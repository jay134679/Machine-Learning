{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Owner: Tian Wang\n",
    "\n",
    "#######################################\n",
    "####Q2.1: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"hw1-data.csv\",\"rb\"),delimiter=\",\",skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test(data):\n",
    "    int(data.shape[0])\n",
    "    int(data.shape[1])\n",
    "    train = data[0:1,:]\n",
    "    test = data[1:2,:]\n",
    "    for i in range(2,int(data.shape[0])):\n",
    "        if random.random() < 0.5:\n",
    "            train = np.concatenate((train,data[i:i+1,:]))\n",
    "        else :\n",
    "            test = np.concatenate((test,data[i:i+1,:]))\n",
    "    return (train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train,test=split_train_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
    "        test  - test set, a 2D numpy array of size (num_instances, num_features)\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized  - test set after normalization\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    parms = {\"min\":[],\"range\":[]}\n",
    "\n",
    "    for i in range(0,int(train.shape[1])):\n",
    "        if min(train[:,i]) == 0 and max(train[:,i]) == 0:\n",
    "            parms['min'].append(0)\n",
    "            parms['range'].append(0)\n",
    "            continue\n",
    "        else:\n",
    "            parms['min'].append(min(train[:,i]))\n",
    "            parms['range'].append(max(train[:,i])-min(train[:,i]))\n",
    "\n",
    "\n",
    "    for i in range(0,int(test.shape[1])):\n",
    "        if i ==0:\n",
    "            if parms[\"range\"][i]==0:\n",
    "                test_normalized = (test[:,i]-parms[\"min\"][i])\n",
    "            else:\n",
    "                test_normalized = (test[:,i]-parms[\"min\"][i])/parms[\"range\"][i]\n",
    "        else:\n",
    "            if parms[\"range\"][i]==0:\n",
    "                test_normalized = np.vstack((test_normalized,(test[:,i]-parms[\"min\"][i]))) \n",
    "            else:\n",
    "                test_normalized = np.vstack((test_normalized,((test[:,i]-parms[\"min\"][i])/parms[\"range\"][i])))\n",
    "    test_normalized = np.transpose(test_normalized)\n",
    "\n",
    "\n",
    "                \n",
    "    for i in range(0,int(train.shape[1])):\n",
    "        if i ==0:\n",
    "            if parms[\"range\"][i]==0:\n",
    "                train_normalized = (train[:,i]-parms[\"min\"][i])\n",
    "            else:\n",
    "                train_normalized = (train[:,i]-parms[\"min\"][i])/parms[\"range\"][i]\n",
    "        else:\n",
    "            if parms[\"range\"][i]==0:\n",
    "                train_normalized = np.vstack((train_normalized,(train[:,i]-parms[\"min\"][i]))) \n",
    "            else:\n",
    "                train_normalized = np.vstack((train_normalized,((train[:,i]-parms[\"min\"][i])/parms[\"range\"][i])))\n",
    "    train_normalized = np.transpose(train_normalized)\n",
    "    \n",
    "    return (train_normalized,test_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_norm,test_norm=feature_normalization(train, test)\n",
    "theta = np.expand_dims(np.ones(int(train_norm.shape[1])), axis=1)\n",
    "#print a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################\n",
    "####Q2.2a: The square loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "    \n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    #TODO\n",
    "    \n",
    "    part = np.dot(X,theta)-y\n",
    "    loss = loss + np.dot(np.transpose(part),part)\n",
    "    loss = loss/(2*int(y.shape[0]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1191.91,  1187.48,  1174.24,  1175.96,  1166.48,  1154.23,\n",
       "        1137.73,  1146.97,  1138.78,  1140.55,  1161.68,  1166.7 ,\n",
       "        1169.82,  1163.83,  1159.71,  1159.32,  1152.92,  1156.33,\n",
       "        1168.97,  1182.11,  1188.41,  1191.55,  1194.55,  1192.87,\n",
       "        1201.64,  1215.87,  1217.87,  1215.73,  1196.81,  1201.67,\n",
       "        1193.15,  1198.2 ,  1199.82,  1213.93,  1212.88,  1219.18,\n",
       "        1203.8 ,  1221.71,  1224.75,  1223.81,  1230.07,  1226.29,\n",
       "        1223.76,  1207.13,  1206.48,  1208.77,  1196.54,  1185.9 ,\n",
       "        1180.28,  1186.27,  1177.54,  1179.46,  1174.37,  1166.06,\n",
       "        1166.15,  1182.11,  1187.8 ,  1210.82,  1215.06,  1197.54,\n",
       "        1184.7 ,  1188.51,  1179.  ,  1190.37,  1202.63,  1193.14,\n",
       "        1173.53,  1166.51,  1171.39,  1182.66,  1184.58,  1187.31,\n",
       "        1182.93,  1196.65,  1186.27,  1195.84,  1203.63,  1222.34,\n",
       "        1223.18,  1224.89,  1215.24,  1227.21,  1196.48,  1185.11,\n",
       "        1189.19,  1178.32,  1183.43,  1164.78,  1167.67,  1152.69,\n",
       "        1133.35,  1120.03,  1127.83,  1119.9 ,  1096.82,  1103.44,\n",
       "        1105.64,  1099.93,  1085.75,  1088.07,  1071.34,  1091.99,\n",
       "        1106.56,  1090.02,  1102.71,  1104.91,  1102.5 ,  1084.89,\n",
       "        1082.25,  1088.92,  1103.67,  1115.63,  1125.72,  1112.49,\n",
       "        1107.14,  1113.22,  1124.48,  1119.55,  1122.54,  1123.68,\n",
       "        1130.92,  1124.55,  1120.38,  1128.73,  1137.06,  1136.75,\n",
       "        1137.13,  1144.7 ,  1141.39,  1141.2 ,  1138.08,  1143.03,\n",
       "        1123.33,  1104.46,  1096.87,  1097.52,  1082.75,  1081.27,\n",
       "        1095.63,  1088.25,  1087.49,  1104.86,  1090.65,  1084.15,\n",
       "        1080.06,  1077.01,  1078.65,  1082.57,  1069.53,  1067.27,\n",
       "        1064.12,  1056.84,  1076.18,  1053.  ,  1051.98,  1053.04,\n",
       "        1051.27,  1054.45,  1053.53,  1041.26,   994.24,   993.31,\n",
       "         999.7 ,  1002.72,  1012.05,  1009.89,  1028.44,  1028.27,\n",
       "        1022.25,  1018.12,  1020.4 ,  1024.55,  1010.54,  1010.95,\n",
       "         987.78,   976.99,   987.02,   989.29,   989.71,   996.6 ,\n",
       "        1005.42,  1003.03,  1005.37,  1001.72,   979.63,   980.91,\n",
       "         976.8 ,   977.01,   963.03,   961.62,   965.62,   960.99,\n",
       "         963.06,   967.09,   971.36,   966.17,   957.7 ,   942.69,\n",
       "         935.54,   943.38,   936.78,   939.06,   938.09,   911.71,\n",
       "         910.57,   915.7 ,   927.42,   935.51,   933.07,   922.62,\n",
       "         938.46,   959.33,   958.74,   962.83,   966.31,   955.59,\n",
       "         948.71,   950.25,   948.75,   952.4 ,   955.14,   957.48,\n",
       "         974.34,  1040.62,  1039.39,  1033.32,  1034.71,  1039.85,\n",
       "        1032.75,  1035.14,  1051.31,  1062.54,  1062.26,  1056.23,\n",
       "        1049.12,  1054.5 ,  1043.31,  1044.42,  1038.29,  1035.3 ,\n",
       "        1036.53,  1021.96,  1016.36,  1016.32,  1006.73,   999.1 ,\n",
       "         998.63,   994.72])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = np.loadtxt(open(\"Data.csv\",\"rb\"),delimiter=\",\",skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data1[:,0:9]\n",
    "y = data1[:,9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = np.array([[-0.003055,0.604211,0.321124,-0.022503,-0.047455,-0.037271,0.095452,0.309359,0.087039]])\n",
    "theta = np.transpose(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9L, 1L)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08348516]]\n"
     ]
    }
   ],
   "source": [
    "print compute_square_loss(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################\n",
    "###Q2.2b: compute the gradient of square loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute gradient of the square loss (as defined in compute_square_loss), at the point theta.\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "    \n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    grad = np.dot(np.transpose(theta),np.transpose(X))\n",
    "    grad = np.dot(grad,X)\n",
    "    grad = grad - np.dot(np.transpose(y),X)\n",
    "    grad = grad/(int(y.shape[0]))\n",
    "    \n",
    "    return grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.array([[0,0,0,-0,-0,-0,0,0,0]])\n",
    "theta = np.transpose(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.array([[-0.003055076,0.604211200,0.321123595,-0.022502569,-0.047454715,-0.037270900,0.095451991,0.309358974,0.087038807]])\n",
    "theta = np.transpose(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.57329400e-04  -4.46819019e-06  -3.00018156e-06  -2.67514120e-06\n",
      "   -3.90733370e-05  -5.38712805e-06  -7.14089849e-07  -6.06772530e-07\n",
      "   -2.36076641e-06]]\n"
     ]
    }
   ],
   "source": [
    "print compute_square_loss_gradient(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################\n",
    "###Q2.3a: Gradient Checker\n",
    "#Getting the gradient calculation correct is often the trickiest part\n",
    "#of any gradient-based optimization algorithm.  Fortunately, it's very\n",
    "#easy to check that the gradient calculation is correct using the\n",
    "#definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4): \n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions: \n",
    "    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1) \n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by: \n",
    "    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "    \n",
    "    Return:\n",
    "        A boolean value indicate whether the gradient is correct or not\n",
    "\n",
    "    \"\"\"\n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #the true gradient\n",
    "    num_features = int(theta.shape[0])\n",
    "    approx_gradient = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    #TODO\n",
    "    for i in range(0,num_features):\n",
    "\n",
    "        theta[i] = theta[i] - epsilon\n",
    "        part1 = compute_square_loss(X, y, theta)\n",
    "        theta[i] = theta[i] + 2 * epsilon\n",
    "        part2 = compute_square_loss(X, y, theta)\n",
    "        approx_gradient[i] = (part2-part1)/(2*epsilon)\n",
    "        theta[i] = theta[i] - epsilon\n",
    "        \n",
    "    for i in range(0,num_features):\n",
    "        if abs(approx_gradient[i]-true_gradient[0][i])<tolerance:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################\n",
    "###Q2.3b: Generic Gradient Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. And check whether gradient_func(X, y, theta) returned\n",
    "    the true gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    \n",
    "    true_gradient = gradient_func(X, y, theta) #the true gradient\n",
    "    num_features = int(theta.shape[0])\n",
    "    approx_gradient = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    #TODO\n",
    "    for i in range(0,num_features):\n",
    "\n",
    "        theta[i] = theta[i] - epsilon\n",
    "        part1 = objective_func(X, y, theta)\n",
    "        theta[i] = theta[i] + 2 * epsilon\n",
    "        part2 = objective_func(X, y, theta)\n",
    "        approx_gradient[i] = (part2-part1)/(2*epsilon)\n",
    "        theta[i] = theta[i] - epsilon\n",
    "\n",
    "    for i in range(0,num_features):\n",
    "        if abs(approx_gradient[i]-true_gradient[0][i])<tolerance:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_gradient_checker(X, y, theta, compute_square_loss, compute_square_loss_gradient, epsilon=0.01, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################################\n",
    "####Q2.4a: Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train,test = split_train_test(data1)\n",
    "train_norm, test_norm = feature_normalization(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = train_norm[:,0:9]\n",
    "b = np.expand_dims(np.ones(int(X.shape[0])), axis=1)\n",
    "X = np.hstack((X,b))\n",
    "y = train_norm[:,9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_grad_descent(X, y, theta, alpha=0.1, num_iter=1000, check_gradient=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the square loss objective\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_iter - number of iterations to run \n",
    "        check_gradient - a boolean value indicating whether checking the gradient when updating\n",
    "        \n",
    "    Returns:\n",
    "        theta_hist - store the the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features) \n",
    "                    for instance, theta in iteration 0 should be theta_hist[0], theta in ieration (num_iter) is theta_hist[-1]\n",
    "        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1) \n",
    "    \"\"\"\n",
    "    num_instances, num_features = int(X.shape[0]), int(X.shape[1])\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #initialize loss_hist\n",
    "    theta = np.expand_dims(np.ones(num_features), axis=1) #initialize theta\n",
    "    #TODO\n",
    "    \n",
    "    if check_gradient==True:\n",
    "        c = generic_gradient_checker(X, y, theta, compute_square_loss, compute_square_loss_gradient, epsilon=0.01, tolerance=1e-4)\n",
    "        if c == True:\n",
    "            for i in range(0,num_iter+1):    \n",
    "                loss_hist[i] =  loss_hist[i]+compute_square_loss(X, y, theta)[0]\n",
    "                theta_hist[i] = theta_hist[i]+np.transpose(theta)[0]\n",
    "                g = compute_square_loss_gradient(X, y, theta) \n",
    "                if i >3 :\n",
    "                    if sum(abs(theta_hist[i]-theta_hist[i-2]))/num_features<1e-3:\n",
    "                        print (\"iteration: %s times\" % (i))\n",
    "                        return (theta_hist,loss_hist)               \n",
    "                \n",
    "                g[0]= g[0]/math.sqrt(sum(np.multiply(g[0],g[0])))\n",
    "                theta = theta-np.transpose(g)*alpha\n",
    "\n",
    "            return (theta_hist,loss_hist)\n",
    "        else:\n",
    "            return (None,None)\n",
    "    else:\n",
    "        for i in range(0,num_iter+1):    \n",
    "            loss_hist[i] =  loss_hist[i]+compute_square_loss(X, y, theta)[0]\n",
    "            theta_hist[i] = theta_hist[i]+np.transpose(theta)[0]\n",
    "            g = compute_square_loss_gradient(X, y, theta)\n",
    "            if i >3 :\n",
    "                if sum(abs(theta_hist[i]-theta_hist[i-2]))/num_features<1e-3:\n",
    "                    print (\"iteration: %s times\" % (i))\n",
    "                    return (theta_hist,loss_hist)  \n",
    "            g[0]= g[0]/math.sqrt(sum(np.multiply(g[0],g[0])))\n",
    "            theta = theta-np.transpose(g)*alpha\n",
    "\n",
    "        return (theta_hist,loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 857 times\n"
     ]
    }
   ],
   "source": [
    "h_theta,h_l=batch_grad_descent(X, y, alpha=0.01, num_iter=1000, check_gradient=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.69997803e-04,   3.68835239e-01,   2.73713723e-01,\n",
       "         1.60426581e-01,  -1.96103840e-01,   1.07945297e-02,\n",
       "         8.54045841e-03,   9.17103811e-02,   1.55418216e-01,\n",
       "        -1.85271272e-02])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_theta[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(h_l[400:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################################\n",
    "###Q2.4b: Implement backtracking line search in batch_gradient_descent\n",
    "###Check http://en.wikipedia.org/wiki/Backtracking_line_search for details\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backtracking_line_search(X,y,theta,p,c=0.5,tao=0.5,num_iter=1000,alpha=10):\n",
    "    t = np.dot(p,compute_square_loss_gradient(X, y, theta)[0])[0]\n",
    "    t = -t * c\n",
    "    \n",
    "    for j in range(0,num_iter):\n",
    "        if (compute_square_loss(X, y, theta)-compute_square_loss(X, y, (theta+alpha*np.transpose(p))))>alpha*t:\n",
    "            return alpha\n",
    "        else:\n",
    "            alpha=alpha*tao\n",
    "    print \"not finding within iter\"\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.expand_dims(h_theta[-1], axis=1) #initialize theta\n",
    "p=compute_square_loss_gradient(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backtracking_further(X,y,theta,p,num_iter=10000):\n",
    "    for i in range(0,num_iter):\n",
    "        al = backtracking_line_search(X,y,theta,p,c=0.5,tao=0.5,num_iter=1000,alpha=1e-9)\n",
    "        theta = theta-alpha*np.transpose(p)\n",
    "        p=compute_square_loss_gradient(X, y, theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jj = backtracking_further(X,y,theta,p,num_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###################################################\n",
    "###Q2.5a: Compute the gradient of Regularized Batch Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized square loss function given X, y and theta\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "    \n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    grad = np.dot(np.transpose(theta),np.transpose(X))\n",
    "    grad = np.dot(grad,X)\n",
    "    grad = grad - np.dot(np.transpose(y),X)\n",
    "    grad = grad/(int(y.shape[0]))\n",
    "    grad = grad + 2 * lambda_reg * np.transpose(theta)\n",
    "    \n",
    "    return grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.expand_dims(np.ones(X.shape[1]), axis=1) #initialize theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_norm[:,0:9]\n",
    "b = np.expand_dims(np.ones(int(X.shape[0])), axis=1)\n",
    "X = np.hstack((X,b))\n",
    "y = train_norm[:,9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.97523073,  4.9562603 ,  4.59638379,  4.72663646,  4.38927439,\n",
       "         4.72905532,  3.8421146 ,  4.3638974 ,  4.8588193 ,  6.85673934]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_regularized_square_loss_gradient(X, y, theta, lambda_reg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################\n",
    "###Q2.5b: Batch Gradient Descent with regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "    \n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    #TODO\n",
    "    \n",
    "    part = np.dot(X,theta)-y\n",
    "    loss = loss + np.dot(np.transpose(part),part)\n",
    "    loss = loss/(2*int(y.shape[0]))\n",
    "    loss = loss + lambda_reg *  np.dot(np.transpose(theta),theta)[0]\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularized_grad_descent(X, y, theta, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        lambda_reg - the regularization coefficient\n",
    "        numIter - number of iterations to run \n",
    "        \n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features) \n",
    "        loss_hist - the history of regularized loss value, 1D numpy array\n",
    "    \"\"\"\n",
    "    num_instances, num_features = int(X.shape[0]), int(X.shape[1])\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #initialize loss_hist\n",
    "    theta = np.expand_dims(np.ones(num_features), axis=1) #initialize theta\n",
    "    #TODO\n",
    "    for i in range(0,num_iter+1):    \n",
    "        loss_hist[i] =  loss_hist[i]+compute_regularized_square_loss(X, y, theta, lambda_reg)[0]\n",
    "        theta_hist[i] = theta_hist[i]+np.transpose(theta)[0]\n",
    "        g = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "        g[0]= g[0]/math.sqrt(sum(np.multiply(g[0],g[0])))\n",
    "        theta = theta-np.transpose(g)*alpha\n",
    "\n",
    "    return (theta_hist,loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a,b= regularized_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97974482,  0.97998011,  0.98020227,  0.98049959,  0.98127342,\n",
       "        0.98187444,  0.98258692,  0.98291303,  0.98404658,  0.98592979,\n",
       "        0.98733536,  0.98809273,  0.98974305,  0.99152843,  0.99347988,\n",
       "        0.99382115,  0.99527347,  0.99756935,  0.98219701,  0.98219701,\n",
       "        0.98219701,  0.98365787,  0.98365787,  0.98365787,  0.98485586,\n",
       "        0.98485586,  0.98485586,  0.98540977,  0.98540977,  0.98540977,\n",
       "        0.98571674,  0.98571674,  0.98571674,  0.99106   ,  0.99106   ,\n",
       "        0.99106   ,  0.98909769,  0.98909769,  0.98909769,  0.98796722,\n",
       "        0.98796722,  0.98796722,  0.98745262,  0.98745262,  0.98745262,\n",
       "        0.98716986,  0.98716986,  0.98716986,  0.97964921])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xt = test_norm[:,0:48]\n",
    "b = np.expand_dims(np.ones(int(Xt.shape[0])), axis=1)\n",
    "Xt = np.hstack((Xt,b))\n",
    "yt = test_norm[:,48:49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_minimizer_lambda(X,y,Xt,yt,theta, lambda_start_log = -7.0,lambda_end_log = 3.0,num_iter=10):\n",
    "    for i in range(0,10):\n",
    "        loss={\"train\":[],\"valid\":[]}\n",
    "        r = (lambda_end_log-lambda_start_log)/10\n",
    "        lambda_range = []\n",
    "        plots = []\n",
    "        for i in range(0,num_iter):\n",
    "            lambda_range.append(math.exp(math.log(10)*(lambda_start_log+r*i)))\n",
    "        for i in lambda_range:\n",
    "            a,b=regularized_grad_descent(X, y,theta,  alpha=0.1, lambda_reg=i, num_iter=1000)\n",
    "            loss[\"train\"].append(compute_regularized_square_loss(X, y, np.transpose(a[-2:-1]),i)[0])\n",
    "            loss[\"valid\"].append(compute_square_loss(Xt, yt, np.transpose(a[-2:-1]))[0])\n",
    "\n",
    "        minl = loss[\"valid\"].index(min(loss[\"valid\"]))\n",
    "\n",
    "        if minl == 0:\n",
    "            lambda_end_log = lambda_start_log+r\n",
    "        elif minl == len(lambda_range)-1:\n",
    "            lambda_start_log = lambda_end_log-r\n",
    "        else:\n",
    "            if loss[\"valid\"][minl-1]<loss[\"valid\"][minl+1]:\n",
    "                lambda_start_log = lambda_start_log + (minl-1)*r\n",
    "                lambda_end_log = lambda_start_log + r\n",
    "            else:\n",
    "                lambda_start_log = lambda_start_log + (minl)*r\n",
    "                lambda_end_log = lambda_start_log + r\n",
    "\n",
    "    return lambda_range[loss[\"valid\"].index(min(loss[\"valid\"]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_lambda = find_minimizer_lambda(lambda_start_log = -7.0,lambda_end_log = 3.0,num_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Once you have found a good value for λ, repeat the fits with different values for\n",
    "B, and plot the results. For this dataset, does regularizing the bias help, hurt, or make no\n",
    "significant difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_r = []\n",
    "loss_b = []\n",
    "for i in range(-5,5):\n",
    "    X = train_norm[:,0:9]\n",
    "    b = np.expand_dims(np.ones(int(X.shape[0])), axis=1) * math.pow(10,i)\n",
    "    X = np.hstack((X,b))\n",
    "    y = train_norm[:,9:10]\n",
    "    h_theta,h_loss= regularized_grad_descent(X, y, alpha=0.1, lambda_reg=best_lambda, num_iter=1000)\n",
    "    h_theta1,h_loss1= batch_grad_descent(X, y, alpha=0.1, num_iter=1000, check_gradient=False)\n",
    "    loss_r.append(h_loss[-1])\n",
    "    loss_b.append(h_loss1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_r,loss_b)\n",
    "plt.xticks(range(10), range(-5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Estimate the average time it takes on your computer to compute a single gradient\n",
    "step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.99999523163e-05 seconds per gradient---\n"
     ]
    }
   ],
   "source": [
    "n=1000\n",
    "start_time = time.time()\n",
    "regularized_grad_descent(X, y, alpha=0.1, lambda_reg=best_lambda, num_iter=n)\n",
    "elasp = (time.time() - start_time)\n",
    "print(\"--- %s seconds per gradient---\" % (elasp/n) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################\n",
    "###Q2.6a: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_norm[:,0:48]\n",
    "b = np.expand_dims(np.ones(int(X.shape[0])), axis=1)\n",
    "X = np.hstack((X,b))\n",
    "y = train_norm[:,48:49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(train,loss_f,grad_f, alpha, lambda_reg, reshuffling=False, num_iter=1000,alpha_sqrt=False,B=1):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with a regularization term\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - string or float. step size in gradient descent\n",
    "                NOTE: In SGD, it's not always a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every iteration is alpha.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t)\n",
    "                if alpha == \"1/t\", alpha = 1/t\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_iter - number of epochs (i.e number of times) to go through the whole training set\n",
    "    \n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features) \n",
    "        loss hist - the history of regularized loss function vector, 2D numpy array of size(num_iter, num_instances)\n",
    "    \"\"\"\n",
    "        \n",
    "    train1 = train\n",
    "    X = train1[:,0:48]\n",
    "    b = np.expand_dims(np.ones(int(X.shape[0])), axis=1) *B \n",
    "    X = np.hstack((X,b))\n",
    "    y = train1[:,48:49]\n",
    "        \n",
    "    num_instances, num_features = int(X.shape[0]), int(X.shape[1])\n",
    "    theta = np.expand_dims(np.ones(num_features), axis=1) #initialize theta\n",
    "    g_lag = 0\n",
    "    theta_hist = np.zeros((num_iter, num_instances, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros((num_iter, num_instances)) #Initialize loss_hist\n",
    "    \n",
    "    alpha =  alpha / (1+ alpha *lambda_reg)\n",
    "    \n",
    "    for i in range(0,num_iter):  \n",
    "        #if alpha_sqrt==False: alpha=1.0/(i+1)\n",
    "        #else: alpha=1.0/math.sqrt(i+1)\n",
    "        for j in range(0,num_instances):\n",
    "            loss_hist[i][j] =  loss_hist[i][j]+loss_f(X[j:j+1], y[j:j+1], theta, lambda_reg)[0]\n",
    "            theta_hist[i][j] = theta_hist[i][j]+np.transpose(theta)[0]\n",
    "            g = grad_f(X[j:j+1], y[j:j+1], theta, lambda_reg)\n",
    "            if j > 3:\n",
    "                if sum(np.transpose(abs(g+g_lag))*alpha)/num_features<1e-5:\n",
    "                    print (\"iteration: %s times; now on %sth instance\" % (i,j))\n",
    "                    return (theta_hist,loss_hist)\n",
    "            g[0]= g[0]/math.sqrt(sum(np.multiply(g[0],g[0])))\n",
    "            theta = theta-np.transpose(g)*alpha\n",
    "            g_lag = g\n",
    "\n",
    "        if reshuffling==True:\n",
    "            np.random.shuffle(train1)\n",
    "            X = train1[:,0:48]\n",
    "            b = np.expand_dims(np.ones(int(X.shape[0])), axis=1)*B \n",
    "            X = np.hstack((X,b))\n",
    "            y = train1[:,48:49]\n",
    "    return (theta_hist,loss_hist)\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1383 times; now on 58th instance\n",
      "--- 13.0529999733 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "a,b=stochastic_grad_descent(train_norm, alpha=0.1, lambda_reg=best_lambda,reshuffling=False, num_iter=10000,alpha_sqrt=False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 92.6349999905 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "a,b=stochastic_grad_descent(train_norm, alpha=0.1, lambda_reg=best_lambda,reshuffling=True, num_iter=10000,alpha_sqrt=False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 114 times\n",
      "--- 0.0139999389648 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "a,b=batch_grad_descent(X, y, alpha=0.1, num_iter=1000, check_gradient=False)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 71 times; now on 70th instance\n"
     ]
    }
   ],
   "source": [
    "a,b=stochastic_grad_descent(train, compute_regularized_square_loss,compute_regularized_square_loss_gradient,alpha=0.1 , lambda_reg=best_lambda,reshuffling=False, num_iter=1000,alpha_sqrt=False,B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83316732,  1.50220708,  1.08556428,  0.40782899,  0.24043455,\n",
       "       -0.21588492,  0.32424505,  0.5386682 ,  1.73562373,  0.64277362,\n",
       "        0.53705342, -0.2034212 , -0.21727123, -0.18997313,  1.1980233 ,\n",
       "        0.97219928,  0.88458695,  1.27520022,  0.7156433 ,  0.7156433 ,\n",
       "       -0.4217781 ,  0.63616279,  0.63616279, -0.81918067,  0.64641752,\n",
       "        0.64641752, -0.76790703,  0.67546741,  0.67546741, -0.62265755,\n",
       "        0.70591098,  0.70591098, -0.47043971,  0.70044703,  0.70044703,\n",
       "       -0.49775945,  0.73042004,  0.73042004, -0.34789443,  0.76794635,\n",
       "        0.76794635, -0.16026287,  0.79929185,  0.79929185, -0.00353535,\n",
       "        0.82420764,  0.82420764,  0.12104358,  0.82838745])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[72][70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b=stochastic_grad_descent(train, compute_regularized_square_loss,compute_regularized_square_loss_gradient,alpha=0.1 , lambda_reg=best_lambda,reshuffling=False, num_iter=1000,alpha_sqrt=False,B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 440 times; now on 23th instance\n"
     ]
    }
   ],
   "source": [
    "a,b=stochastic_grad_descent(train, compute_regularized_square_loss,compute_regularized_square_loss_gradient,alpha=0.05 , lambda_reg=best_lambda,reshuffling=False, num_iter=1000,alpha_sqrt=False,B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 61 times; now on 52th instance\n"
     ]
    }
   ],
   "source": [
    "a,b=stochastic_grad_descent(train, compute_regularized_square_loss,compute_regularized_square_loss_gradient,alpha=0.005 , lambda_reg=best_lambda,reshuffling=False, num_iter=1000,alpha_sqrt=False,B=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a stepsize rule of the form ηt = 1+η η0 0λt, where λ is your regularization constant,\n",
    "and η0 a constant you can choose. How do the results compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b=stochastic_grad_descent(train, compute_regularized_square_loss,compute_regularized_square_loss_gradient,alpha=0.1 , lambda_reg=best_lambda,reshuffling=False, num_iter=1000,alpha_sqrt=False,B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 440 times; now on 23th instance\n"
     ]
    }
   ],
   "source": [
    "a,b=stochastic_grad_descent(train, compute_regularized_square_loss,compute_regularized_square_loss_gradient,alpha=0.05 , lambda_reg=best_lambda,reshuffling=False, num_iter=1000,alpha_sqrt=False,B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 61 times; now on 52th instance\n"
     ]
    }
   ],
   "source": [
    "a,b=stochastic_grad_descent(train, compute_regularized_square_loss,compute_regularized_square_loss_gradient,alpha=0.005 , lambda_reg=best_lambda,reshuffling=False, num_iter=1000,alpha_sqrt=False,B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
