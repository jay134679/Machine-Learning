{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation part: run all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "    \n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    #TODO\n",
    "    \n",
    "    part = np.dot(X,theta)-y\n",
    "    loss = loss + np.dot(np.transpose(part),part)\n",
    "    loss = loss/(2*int(y.shape[0]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "    \n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    #TODO\n",
    "    \n",
    "    part = np.dot(X,theta)-y\n",
    "    loss = loss + np.dot(np.transpose(part),part)\n",
    "    loss = loss/(2*int(y.shape[0]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute gradient of the square loss (as defined in compute_square_loss), at the point theta.\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "    \n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    grad = np.dot(np.transpose(theta),np.transpose(X))\n",
    "    grad = np.dot(grad,X)\n",
    "    grad = grad - np.dot(np.transpose(y),X)\n",
    "    grad = grad/(int(y.shape[0]))\n",
    "    \n",
    "    return grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_grad_descent(X, y, theta, alpha=0.1, num_iter=1000, check_gradient=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the square loss objective\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_iter - number of iterations to run \n",
    "        check_gradient - a boolean value indicating whether checking the gradient when updating\n",
    "        \n",
    "    Returns:\n",
    "        theta_hist - store the the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features) \n",
    "                    for instance, theta in iteration 0 should be theta_hist[0], theta in ieration (num_iter) is theta_hist[-1]\n",
    "        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1) \n",
    "    \"\"\"\n",
    "    num_instances, num_features = int(X.shape[0]), int(X.shape[1])\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #initialize loss_hist\n",
    "    theta = np.expand_dims(np.ones(num_features), axis=1) #initialize theta\n",
    "    #TODO\n",
    "    \n",
    "    if check_gradient==True:\n",
    "        c = generic_gradient_checker(X, y, theta, compute_square_loss, compute_square_loss_gradient, epsilon=0.01, tolerance=1e-4)\n",
    "        if c == True:\n",
    "            for i in range(0,num_iter+1):    \n",
    "                loss_hist[i] =  loss_hist[i]+compute_square_loss(X, y, theta)[0]\n",
    "                theta_hist[i] = theta_hist[i]+np.transpose(theta)[0]\n",
    "                g = compute_square_loss_gradient(X, y, theta) \n",
    "                if i >3 :\n",
    "                    if sum(abs(theta_hist[i]-theta_hist[i-2]))/num_features<1e-3:\n",
    "                        print (\"iteration: %s times\" % (i))\n",
    "                        return (theta_hist,loss_hist)               \n",
    "                \n",
    "                g[0]= g[0]/math.sqrt(sum(np.multiply(g[0],g[0])))\n",
    "                theta = theta-np.transpose(g)*alpha\n",
    "\n",
    "            return (theta_hist,loss_hist)\n",
    "        else:\n",
    "            return (None,None)\n",
    "    else:\n",
    "        for i in range(0,num_iter+1):    \n",
    "            loss_hist[i] =  loss_hist[i]+compute_square_loss(X, y, theta)[0]\n",
    "            theta_hist[i] = theta_hist[i]+np.transpose(theta)[0]\n",
    "            g = compute_square_loss_gradient(X, y, theta)\n",
    "            if i >3 :\n",
    "                if sum(abs(theta_hist[i]-theta_hist[i-2]))/num_features<1e-3:\n",
    "                    print (\"iteration: %s times\" % (i))\n",
    "                    return (theta_hist,loss_hist)  \n",
    "            g[0]= g[0]/math.sqrt(sum(np.multiply(g[0],g[0])))\n",
    "            theta = theta-np.transpose(g)*alpha\n",
    "\n",
    "        return (theta_hist,loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized square loss function given X, y and theta\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "    \n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    grad = np.dot(np.transpose(theta),np.transpose(X))\n",
    "    grad = np.dot(grad,X)\n",
    "    grad = grad - np.dot(np.transpose(y),X)\n",
    "    grad = grad/(int(y.shape[0]))\n",
    "    grad = grad + 2 * lambda_reg * np.transpose(theta)\n",
    "    \n",
    "    return grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "    \n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    #TODO\n",
    "    \n",
    "    part = np.dot(X,theta)-y\n",
    "    loss = loss + np.dot(np.transpose(part),part)\n",
    "    loss = loss/(2*int(y.shape[0]))\n",
    "    loss = loss + lambda_reg *  np.dot(np.transpose(theta),theta)[0]\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularized_grad_descent(X, y,theta,  alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        lambda_reg - the regularization coefficient\n",
    "        numIter - number of iterations to run \n",
    "        \n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features) \n",
    "        loss_hist - the history of regularized loss value, 1D numpy array\n",
    "    \"\"\"\n",
    "    num_instances, num_features = int(X.shape[0]), int(X.shape[1])\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #initialize loss_hist\n",
    "    theta = np.expand_dims(np.ones(num_features), axis=1) #initialize theta\n",
    "    #TODO\n",
    "    for i in range(0,num_iter+1):    \n",
    "        loss_hist[i] =  loss_hist[i]+compute_regularized_square_loss(X, y, theta, lambda_reg)[0]\n",
    "        theta_hist[i] = theta_hist[i]+np.transpose(theta)[0]\n",
    "        g = compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "        g[0]= g[0]/math.sqrt(sum(np.multiply(g[0],g[0])))\n",
    "        theta = theta-np.transpose(g)*alpha\n",
    "\n",
    "    return (theta_hist,loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_l2_minimizer_lambda(X,y,Xt,yt,theta, lambda_start_log = -7.0,lambda_end_log = 3.0,num_iter=10):\n",
    "    for i in range(0,10):\n",
    "        loss={\"train\":[],\"valid\":[]}\n",
    "        r = (lambda_end_log-lambda_start_log)/10\n",
    "        lambda_range = []\n",
    "        plots = []\n",
    "        for i in range(0,num_iter):\n",
    "            lambda_range.append(math.exp(math.log(10)*(lambda_start_log+r*i)))\n",
    "        for i in lambda_range:\n",
    "            a,b=regularized_grad_descent(X, y,theta,  alpha=0.1, lambda_reg=i, num_iter=1000)\n",
    "            loss[\"train\"].append(compute_regularized_square_loss(X, y, np.transpose(a[-2:-1]),i)[0])\n",
    "            loss[\"valid\"].append(compute_square_loss(Xt, yt, np.transpose(a[-2:-1]))[0])\n",
    "\n",
    "        minl = loss[\"valid\"].index(min(loss[\"valid\"]))\n",
    "\n",
    "        if minl == 0:\n",
    "            lambda_end_log = lambda_start_log+r\n",
    "        elif minl == len(lambda_range)-1:\n",
    "            lambda_start_log = lambda_end_log-r\n",
    "        else:\n",
    "            if loss[\"valid\"][minl-1]<loss[\"valid\"][minl+1]:\n",
    "                lambda_start_log = lambda_start_log + (minl-1)*r\n",
    "                lambda_end_log = lambda_start_log + r\n",
    "            else:\n",
    "                lambda_start_log = lambda_start_log + (minl)*r\n",
    "                lambda_end_log = lambda_start_log + r\n",
    "\n",
    "    return lambda_range[loss[\"valid\"].index(min(loss[\"valid\"]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_l1_minimizer_lambda(X,y,Xt,yt,theta, lambda_start_log = -7.0,lambda_end_log = 3.0,num_iter=10):\n",
    "    for i in range(0,10):\n",
    "        loss={\"train\":[],\"valid\":[]}\n",
    "        r = (lambda_end_log-lambda_start_log)/10\n",
    "        lambda_range = []\n",
    "        plots = []\n",
    "        for i in range(0,num_iter):\n",
    "            lambda_range.append(math.exp(math.log(10)*(lambda_start_log+r*i)))\n",
    "        for i in lambda_range:\n",
    "            a,b=coordinate_descent_lasso_vec(X, y, lambda_reg=i, w= theta, num_iter=1000, tol=1e-3)\n",
    "            loss[\"train\"].append(compute_lasso_square_loss(X, y, a,i)[0])\n",
    "            loss[\"valid\"].append(compute_square_loss(Xt, yt, a)[0])\n",
    "\n",
    "        minl = loss[\"valid\"].index(min(loss[\"valid\"]))\n",
    "\n",
    "        if minl == 0:\n",
    "            lambda_end_log = lambda_start_log+r\n",
    "        elif minl == len(lambda_range)-1:\n",
    "            lambda_start_log = lambda_end_log-r\n",
    "        else:\n",
    "            if loss[\"valid\"][minl-1]<loss[\"valid\"][minl+1]:\n",
    "                lambda_start_log = lambda_start_log + (minl-1)*r\n",
    "                lambda_end_log = lambda_start_log + r\n",
    "            else:\n",
    "                lambda_start_log = lambda_start_log + (minl)*r\n",
    "                lambda_end_log = lambda_start_log + r\n",
    "\n",
    "    return lambda_range[loss[\"valid\"].index(min(loss[\"valid\"]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_the_sparsity(V,tol=0.001):\n",
    "    a = 0.0\n",
    "    t = 0.0\n",
    "    for e in V:\n",
    "        t = t + 1\n",
    "        if abs(e) < tol:\n",
    "            a = a + 1\n",
    "            print True\n",
    "    print (\"%s supports against %s elements in total, %s\" % (a,t,a/t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_lasso_square_loss(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "    \n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    #TODO\n",
    "    \n",
    "    part = np.dot(X,theta)-y\n",
    "    loss = loss + np.dot(np.transpose(part),part)\n",
    "    loss = loss/(2*int(y.shape[0]))\n",
    "    loss = loss + lambda_reg *  sum(abs(theta))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coordinate_descent_lasso(X,y,lambda_reg,w,num_iter=1000,tol=1e-3):\n",
    "    # initialize w\n",
    "    n,d = X.shape\n",
    "    n = int(n);d = int(d)\n",
    "    \n",
    "    try:\n",
    "        l = len(w)\n",
    "    except TypeError:\n",
    "        l = 0\n",
    "    \n",
    "    if l == 0:\n",
    "        if w == 0:\n",
    "            w = np.expand_dims(np.zeros(d),axis = 1)    \n",
    "        else:\n",
    "            w = np.dot(np.transpose(X),X)\n",
    "            w = w + lambda_reg * numpy.identity(d)\n",
    "            w = numpy.matrix(w).I\n",
    "            w = np.dot(w,np.transpose(X))\n",
    "            w = np.dot(w,y)\n",
    "\n",
    "            w = np.expand_dims(w.A1,axis = 1)\n",
    "        \n",
    "    w_i = w + 0\n",
    "    w_l = w + 0\n",
    "    \n",
    "    a = np.expand_dims(np.zeros(d), axis=1)\n",
    "    for i in range(0,d):\n",
    "        for j in range(0,n):\n",
    "            a[i] = a[i] + 2*(X[j:(j+1),i:(i+1)] * X[j:(j+1),i:(i+1)])\n",
    "\n",
    "    for ii in range(0,num_iter): \n",
    "        \n",
    "        c = np.expand_dims(np.zeros(d), axis=1)\n",
    "        for i in range(0,d):\n",
    "            for j in range(0,n):\n",
    "\n",
    "                c[i] = c[i] + 2*(X[j:(j+1),i:(i+1)] * (y[j] - np.dot(X[j:(j+1)],w) + w[i] * X[j:(j+1),i:(i+1)]))\n",
    "\n",
    "\n",
    "            if (abs(c[i]/a[i]) - lambda_reg/a[i]) > 0:\n",
    "                w[i] = np.sign(c[i]*a[i]) * (abs(c[i]/a[i]) - lambda_reg / a[i])\n",
    "\n",
    "            else:\n",
    "                w[i] = 0        \n",
    "\n",
    "        if (sum(abs(w)) - sum(abs(w_l)))/d < tol:\n",
    "            print ii\n",
    "            return w,w_i\n",
    "\n",
    "        # print c\n",
    "        w_l = w + 0\n",
    "        \n",
    "    print \"not converging until max iteration\",ii+1\n",
    "\n",
    "    return w,w_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Preliminaries\n",
    "1.1 Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(150,75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3698622 ,  0.64839331,  0.76807463, ...,  0.73917348,\n",
       "         0.11692581,  0.10532022],\n",
       "       [ 0.35775099,  0.72842651,  0.97819585, ...,  0.84916247,\n",
       "         0.20325601,  0.05613429],\n",
       "       [ 0.56667802,  0.3916695 ,  0.40301643, ...,  0.43844608,\n",
       "         0.07407487,  0.08077372],\n",
       "       ..., \n",
       "       [ 0.62298625,  0.85273665,  0.17045826, ...,  0.74635817,\n",
       "         0.44461074,  0.88778269],\n",
       "       [ 0.39516059,  0.39495975,  0.45716019, ...,  0.3366181 ,\n",
       "         0.76790098,  0.55159187],\n",
       "       [ 0.7256994 ,  0.70301292,  0.46259039, ...,  0.9641697 ,\n",
       "         0.97032482,  0.20669141]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = np.expand_dims(np.zeros(int(X.shape[1])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,10) : \n",
    "    if i % 2 == 0: theta[i]=-10; \n",
    "    else: theta[i]=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19488139],\n",
       "       [-0.21114157],\n",
       "       [ 0.01662259],\n",
       "       [-0.37462974],\n",
       "       [ 0.31754295],\n",
       "       [ 0.3548649 ],\n",
       "       [ 0.04744648],\n",
       "       [ 0.19077058],\n",
       "       [-0.08587524],\n",
       "       [-0.05740759],\n",
       "       [ 0.72455398],\n",
       "       [ 0.33511134],\n",
       "       [-0.0013784 ],\n",
       "       [-0.17348269],\n",
       "       [ 0.5084437 ],\n",
       "       [ 0.24693755],\n",
       "       [-0.23296642],\n",
       "       [ 0.26557806],\n",
       "       [-0.24238535],\n",
       "       [-0.23036126],\n",
       "       [-0.16842511],\n",
       "       [-0.4604045 ],\n",
       "       [-0.06706664],\n",
       "       [-0.22949561],\n",
       "       [-0.17365589],\n",
       "       [ 0.2657224 ],\n",
       "       [ 0.53485444],\n",
       "       [ 0.5736443 ],\n",
       "       [ 0.2388065 ],\n",
       "       [-0.5508706 ],\n",
       "       [-0.59182746],\n",
       "       [-0.43519256],\n",
       "       [-0.29945879],\n",
       "       [ 0.04885954],\n",
       "       [ 0.06084195],\n",
       "       [ 0.24273091],\n",
       "       [-0.04897566],\n",
       "       [-0.34364291],\n",
       "       [ 0.05348729],\n",
       "       [-0.32926216],\n",
       "       [-0.2545068 ],\n",
       "       [ 0.65062967],\n",
       "       [-0.12737902],\n",
       "       [-0.12753802],\n",
       "       [-0.07947553],\n",
       "       [-0.42817638],\n",
       "       [ 0.1307494 ],\n",
       "       [-0.24088156],\n",
       "       [ 0.35591437],\n",
       "       [-0.05592767],\n",
       "       [ 0.04465435],\n",
       "       [ 0.46485199],\n",
       "       [ 0.16896976],\n",
       "       [-0.0346852 ],\n",
       "       [ 0.03293676],\n",
       "       [ 0.0128173 ],\n",
       "       [-0.08228358],\n",
       "       [ 0.32776911],\n",
       "       [ 0.40643753],\n",
       "       [-0.13375484],\n",
       "       [-0.07170146],\n",
       "       [-0.25140669],\n",
       "       [-0.20756616],\n",
       "       [-0.03364713],\n",
       "       [ 0.10219708],\n",
       "       [-0.47699191],\n",
       "       [-0.03695597],\n",
       "       [-0.78015737],\n",
       "       [-0.1459768 ],\n",
       "       [-0.74379937],\n",
       "       [-0.81375672],\n",
       "       [-0.10750197],\n",
       "       [-0.13326593],\n",
       "       [ 0.43311826],\n",
       "       [ 0.49060006]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(75,1)/math.sqrt(10) # set the variance of dgp of standart norm to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y= np.dot(X,theta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = y + (np.random.randn(150,1)/math.sqrt(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.20810044],\n",
       "       [  1.12585232],\n",
       "       [ -8.19513537],\n",
       "       [ -0.81901926],\n",
       "       [  9.51206732],\n",
       "       [  7.98758843],\n",
       "       [ -6.84413292],\n",
       "       [ -0.45691219],\n",
       "       [ -3.55960407],\n",
       "       [ 16.49845738],\n",
       "       [ -2.986933  ],\n",
       "       [ -1.02166327],\n",
       "       [  9.69877222],\n",
       "       [  4.65514469],\n",
       "       [  0.79368895],\n",
       "       [  5.01196189],\n",
       "       [ 15.8432394 ],\n",
       "       [ -1.38364884],\n",
       "       [ -9.90872743],\n",
       "       [ 14.35689321],\n",
       "       [  2.87195466],\n",
       "       [ 14.73718524],\n",
       "       [  2.33012568],\n",
       "       [ 11.55064922],\n",
       "       [ 19.58704429],\n",
       "       [  2.05453364],\n",
       "       [ -6.38468587],\n",
       "       [  4.79118093],\n",
       "       [-21.39523211],\n",
       "       [  1.10440007],\n",
       "       [ -0.18252006],\n",
       "       [ -8.85282116],\n",
       "       [ -9.00212767],\n",
       "       [ -3.68441537],\n",
       "       [ 11.78770134],\n",
       "       [  2.14123368],\n",
       "       [  2.75847842],\n",
       "       [  4.41945515],\n",
       "       [  3.9456184 ],\n",
       "       [  0.38183429],\n",
       "       [ -9.89221499],\n",
       "       [ -9.17410934],\n",
       "       [  3.54179686],\n",
       "       [  6.81030112],\n",
       "       [ 12.90541846],\n",
       "       [ -0.60180891],\n",
       "       [  2.01715062],\n",
       "       [ 10.78056556],\n",
       "       [ -7.36204164],\n",
       "       [ 10.63418044],\n",
       "       [  9.62426401],\n",
       "       [ -6.21919966],\n",
       "       [ -4.05185556],\n",
       "       [  2.18231398],\n",
       "       [  0.91246035],\n",
       "       [  0.49285588],\n",
       "       [  0.39315516],\n",
       "       [  2.94211458],\n",
       "       [  2.23036732],\n",
       "       [  2.04950263],\n",
       "       [ -0.67096023],\n",
       "       [  9.22769425],\n",
       "       [ -2.03710443],\n",
       "       [ -7.81285759],\n",
       "       [ -2.83245221],\n",
       "       [-13.63763731],\n",
       "       [ 12.47411684],\n",
       "       [ -7.89121526],\n",
       "       [  8.11641772],\n",
       "       [ -4.91303774],\n",
       "       [ -9.0918517 ],\n",
       "       [ 11.72137747],\n",
       "       [  0.65770308],\n",
       "       [  3.16063824],\n",
       "       [ 22.45207221],\n",
       "       [ -6.77808732],\n",
       "       [  0.45293806],\n",
       "       [  4.64707684],\n",
       "       [  2.50303215],\n",
       "       [  8.1043645 ],\n",
       "       [ 13.33513481],\n",
       "       [  5.79393354],\n",
       "       [  0.78401301],\n",
       "       [-10.14502319],\n",
       "       [ -1.5945972 ],\n",
       "       [  1.50646425],\n",
       "       [ -0.08192038],\n",
       "       [  6.37289102],\n",
       "       [-13.39708642],\n",
       "       [  4.00608418],\n",
       "       [  2.2225352 ],\n",
       "       [ 13.40759345],\n",
       "       [  0.66693706],\n",
       "       [ -5.60692995],\n",
       "       [  8.60338185],\n",
       "       [ -1.23503909],\n",
       "       [-12.58179566],\n",
       "       [-12.81244062],\n",
       "       [ -0.98148089],\n",
       "       [ 11.00041236],\n",
       "       [-12.6914112 ],\n",
       "       [ -0.23118346],\n",
       "       [ -3.94812482],\n",
       "       [  9.62168472],\n",
       "       [-23.66160536],\n",
       "       [ -0.24932354],\n",
       "       [ -7.63896778],\n",
       "       [ 19.27215935],\n",
       "       [  0.71781455],\n",
       "       [ -1.92710964],\n",
       "       [  4.34879701],\n",
       "       [  9.07204906],\n",
       "       [ -8.74374579],\n",
       "       [ -7.24169192],\n",
       "       [ -6.34640024],\n",
       "       [ -0.66972661],\n",
       "       [  9.25595406],\n",
       "       [ -5.24301645],\n",
       "       [  4.39395403],\n",
       "       [ 19.70078072],\n",
       "       [  7.96373713],\n",
       "       [ 11.10963474],\n",
       "       [-12.85218089],\n",
       "       [ 11.10507759],\n",
       "       [  6.23446469],\n",
       "       [-15.05830446],\n",
       "       [ -4.93772082],\n",
       "       [ -0.03190135],\n",
       "       [  8.27323681],\n",
       "       [ 11.34446336],\n",
       "       [ -9.16741423],\n",
       "       [-13.23252937],\n",
       "       [ -9.88788261],\n",
       "       [ 10.35536101],\n",
       "       [-12.5894853 ],\n",
       "       [  2.46807201],\n",
       "       [ -4.29113563],\n",
       "       [  1.37533401],\n",
       "       [-11.96192281],\n",
       "       [  7.34824953],\n",
       "       [ 15.8544632 ],\n",
       "       [  6.26821194],\n",
       "       [  4.23755219],\n",
       "       [ -6.57384174],\n",
       "       [-11.65110774],\n",
       "       [ -3.15441766],\n",
       "       [  1.88573817],\n",
       "       [ -4.44027684],\n",
       "       [  7.23221148],\n",
       "       [ -2.82463292]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = y[0:80]\n",
    "vld_y = y[80:100]\n",
    "test_y = y[100:150]\n",
    "train_X = X[0:80]\n",
    "vld_X = X[80:100]\n",
    "test_X = X[100:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Experiments with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_ini = np.expand_dims(np.ones(int(X.shape[1])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_lambda = find_minimizer_lambda(train_X,train_y,vld_X,vld_y,theta_ini,lambda_start_log = -7.0,lambda_end_log = 3.0,num_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6749657078371515e-07"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.653000116348 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "coef1 = regularized_grad_descent(train_X, train_y,theta_ini,  alpha=0.1, lambda_reg=best_lambda, num_iter=10000)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.38753494e+00,   8.15590868e+00,  -8.17686538e+00,\n",
       "         9.05637686e+00,  -1.04030832e+01,   7.46296821e+00,\n",
       "        -1.06739897e+01,   8.76678924e+00,  -1.11077215e+01,\n",
       "         9.61528510e+00,   8.72504318e-01,   4.56836829e-01,\n",
       "         1.16098629e+00,   1.23731276e+00,   2.48691509e-01,\n",
       "         1.30259655e+00,  -6.52481443e-01,  -2.33136987e-01,\n",
       "         1.18979248e+00,  -1.12747874e+00,   4.70307793e-01,\n",
       "        -1.00145381e+00,   1.08420134e-03,   4.09434792e-03,\n",
       "         8.17129369e-01,   2.28373839e+00,   4.39716700e-01,\n",
       "        -2.06347186e-01,   2.87227842e-01,  -9.99136236e-01,\n",
       "        -1.71869808e+00,   4.28988262e-02,   8.73364734e-01,\n",
       "         3.63919247e-01,  -1.30714258e+00,   1.03902023e-01,\n",
       "        -3.34475789e-01,  -2.60158018e-01,   1.45030733e+00,\n",
       "         2.82418553e-01,  -4.79689780e-02,   1.35114104e+00,\n",
       "         5.64745235e-01,   6.60975862e-01,   1.80067029e-01,\n",
       "         5.99514237e-01,   6.35384112e-01,   6.85818769e-01,\n",
       "        -2.29584127e+00,   1.21304323e+00,   1.55425584e+00,\n",
       "        -2.72091108e-01,   7.01566300e-01,  -1.96744250e+00,\n",
       "        -6.00280256e-01,  -2.61126184e-01,   7.74909343e-01,\n",
       "        -8.83683734e-02,  -8.16333942e-01,   3.62938236e-01,\n",
       "        -2.14594626e-02,  -1.15999690e+00,   3.53195732e-01,\n",
       "         6.02264350e-01,  -2.27016466e+00,  -1.37860709e+00,\n",
       "        -2.93103270e-01,  -1.51262010e-01,  -6.86042428e-01,\n",
       "        -1.02623676e-01,   9.61678889e-01,  -1.07455836e-01,\n",
       "         3.20091376e-01,   1.56561554e+00,  -3.77181984e-01])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef1[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefb = batch_grad_descent(train_X, train_y, theta_ini, alpha=0.1, num_iter=1000, check_gradient=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.70931288,  7.36406523, -6.89141708,  9.18381806, -8.28091154,\n",
       "        5.81122649, -8.10270854,  5.63811386, -9.95551567,  6.25629457,\n",
       "        0.73096707, -0.62755444,  1.00608417,  1.51760554,  0.2515953 ,\n",
       "       -0.86532725, -0.82982022, -0.32418693,  3.10827232, -0.9819107 ,\n",
       "        1.93175779, -0.64760849,  1.99727694, -1.13936216,  0.270002  ,\n",
       "        0.86710222,  0.79002471, -0.77011099,  0.38067527, -1.88886218,\n",
       "       -1.63554336,  0.55065784,  1.83911768,  1.61138311, -1.03200312,\n",
       "        1.4700127 , -0.75707283,  0.12597957,  1.01489577, -0.86319856,\n",
       "       -0.08304562,  0.54676865, -0.99429005,  2.72758883,  0.72766969,\n",
       "        0.39819359,  0.49282983,  1.44970108, -1.69322594,  0.66364001,\n",
       "        0.50981739, -0.47497805,  2.16325113, -2.62515533, -1.44844469,\n",
       "       -0.7428398 ,  0.02028832, -2.29331361, -1.02813626, -0.28171115,\n",
       "        0.42739345, -1.05113648,  0.65521883,  0.22934128, -2.60619894,\n",
       "       -1.37399944,  1.11689229,  0.46363601, -1.92531676,  2.09213698,\n",
       "        2.24510468, -1.26010017,  0.48339828,  1.25504355,  0.39174707])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefb[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in coef1[0][-1][10:]:\n",
    "     if abs(i) < 10e-2:\n",
    "            print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Ridge(alpha=0.1, fit_intercept=False, normalize=False, copy_X=True, max_iter=10000, \n",
    "                           tol=0.001, solver='auto', random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.00100016593933 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "clf.fit(train_X, train_y) \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.78036087,   7.83126148,  -7.65493605,   9.03018743,\n",
       "        -9.64010326,   6.90852825,  -9.81799675,   7.64403118,\n",
       "       -10.65555142,   8.42000558,   0.80135402,   0.09072341,\n",
       "         1.16574773,   1.24686111,   0.22985519,   0.63587797,\n",
       "        -0.74221385,  -0.28221061,   1.85731528,  -1.07502068,\n",
       "         1.04766901,  -0.88558411,   0.68054148,  -0.40885002,\n",
       "         0.59998705,   1.82974375,   0.53791519,  -0.48043933,\n",
       "         0.31658172,  -1.21789313,  -1.6639058 ,   0.26273569,\n",
       "         1.22557968,   0.71535627,  -1.15971798,   0.59085136,\n",
       "        -0.51298077,  -0.0817734 ,   1.3173481 ,  -0.03076678,\n",
       "        -0.01124342,   1.00789705,  -0.02534725,   1.26479906,\n",
       "         0.37975918,   0.54919602,   0.5120877 ,   0.98634309,\n",
       "        -2.02997435,   1.00995031,   1.10579658,  -0.40843186,\n",
       "         1.18956771,  -2.16517309,  -0.92742071,  -0.28318507,\n",
       "         0.47013218,  -0.86623007,  -0.92587959,   0.08463336,\n",
       "         0.07038442,  -1.10749901,   0.43189232,   0.52288915,\n",
       "        -2.32194873,  -1.29491992,   0.20348944,   0.05876612,\n",
       "        -1.06785749,   0.57239551,   1.36516487,  -0.49238285,\n",
       "         0.36581783,   1.42984117,  -0.17111041])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in clf.coef_[0][10:]:\n",
    "     if abs(i) < 10e-3:\n",
    "            print True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Coordinate Descent for Lasso (a.k.a. The Shooting algorithm)\n",
    "2.1 Experiments with the Shooting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "--- 1.85700011253 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "wa,wi = coordinate_descent_lasso(train_X,train_y,lambda_reg=0.01,w=1,num_iter=1000,tol=1e-10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "4.0 supports against 75.0 elements in total, 0.0533333333333\n"
     ]
    }
   ],
   "source": [
    "analyze_the_sparsity(wa,tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_max_lambda_homotopy(X,y):\n",
    "    l = abs(np.dot(np.transpose(X),y))\n",
    "    l_m = l[0]\n",
    "    for i in l:\n",
    "        if i > l_m:\n",
    "            l_m =i\n",
    "    return 2 * l_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def homotopy_method(X, y, lambda_reg, coh=10,tole=1e-10):\n",
    "    l_m = find_max_lambda_homotopy(X,y)\n",
    "    intr = math.log(l_m / lambda_reg) / coh\n",
    "    for i in range(0,coh+1):\n",
    "        m1 = math.exp(math.log(max_lambda)-intr*i)\n",
    "        if i == 0:\n",
    "            wa,wi = coordinate_descent_lasso(X,y,lambda_reg=m1,w=0,num_iter=1000,tol=1e-2)\n",
    "        else:\n",
    "            if i == coh :\n",
    "                wa,wi = coordinate_descent_lasso(X,y,lambda_reg=m1,w=wa,num_iter=1000,tol=tole)   \n",
    "            else:\n",
    "                wa,wi = coordinate_descent_lasso(X,y,lambda_reg=m1,w=wa,num_iter=1000,tol=1e-2)\n",
    "    return wa   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8\n",
      "5\n",
      "5\n",
      "4\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "56\n",
      "--- 6.20399999619 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.50783839]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "wa = homotopy_method(train_X, train_y, lambda_reg=0.01, coh=10, tole = 1e-10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "compute_square_loss(vld_X, vld_y, wa)\n",
    "\n",
    "# this is slower but gives lower empirical risks however the total number of iterations is not that big compared to ordinary descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "--- 0.688999891281 seconds ---\n",
      "[[ 0.47855106]]\n",
      "[[ 1.16732485]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "wa,wi = coordinate_descent_lasso(train_X,train_y,lambda_reg=0.01,w=1,num_iter=1000,tol=1e-10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print compute_square_loss(vld_X, vld_y, wa)\n",
    "print compute_lasso_square_loss(train_X, train_y, wa, lambda_reg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "--- 1.5720000267 seconds ---\n",
      "[[ 2.73703575]]\n",
      "[[ 1.45436057]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "wa,wi = coordinate_descent_lasso(train_X,train_y,lambda_reg=0.01,w=0,num_iter=1000,tol=1e-10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print compute_square_loss(vld_X, vld_y, wa)\n",
    "print compute_lasso_square_loss(train_X, train_y, wa, lambda_reg=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â€œvectorizationâ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coordinate_descent_lasso_vec(X,y,lambda_reg,w,num_iter=1000,tol=1e-3):\n",
    "    # initialize w\n",
    "    n,d = X.shape\n",
    "    n = int(n);d = int(d)\n",
    "    \n",
    "    try:\n",
    "        l = len(w)\n",
    "    except TypeError:\n",
    "        l = 0\n",
    "    \n",
    "    if l == 0:\n",
    "        if w == 0:\n",
    "            w = np.expand_dims(np.zeros(d),axis = 1)    \n",
    "        else:\n",
    "            w = np.dot(np.transpose(X),X)\n",
    "            w = w + lambda_reg * numpy.identity(d)\n",
    "            w = numpy.matrix(w).I\n",
    "            w = np.dot(w,np.transpose(X))\n",
    "            w = np.dot(w,y)\n",
    "\n",
    "            w = np.expand_dims(w.A1,axis = 1)\n",
    "        \n",
    "    w_i = w + 0\n",
    "    w_l = w + 0\n",
    "    \n",
    "    a = np.expand_dims(np.zeros(d), axis=1)\n",
    "    XX = np.dot(np.transpose(X),X)\n",
    "    \n",
    "    for i in range(0,d):\n",
    "        a[i] = 2* XX[i:(i+1),i:(i+1)]\n",
    "\n",
    "    for ii in range(0,num_iter): \n",
    "        \n",
    "        c = np.expand_dims(np.zeros(d), axis=1)\n",
    "        for i in range(0,d):\n",
    "            c[i] = 2*np.dot(np.transpose(X[:,i:(i+1)]) , (y - np.dot(X,w) + w[i] * X[:,i:(i+1)]))\n",
    "            \n",
    "            if (abs(c[i]/a[i]) - lambda_reg/a[i]) > 0:\n",
    "                w[i] = np.sign(c[i]*a[i]) * (abs(c[i]/a[i]) - lambda_reg / a[i])\n",
    "\n",
    "            else:\n",
    "                w[i] = 0        \n",
    "\n",
    "        if (sum(abs(w)) - sum(abs(w_l)))/d < tol:\n",
    "            #print ii\n",
    "            return w,w_i\n",
    "        # print c\n",
    "        w_l = w + 0\n",
    "        \n",
    "    print \"not converging until max iteration\",ii+1\n",
    "\n",
    "    return w,w_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "--- 0.280999898911 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.24052104]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "wa,wi = coordinate_descent_lasso_vec(train_X,train_y,lambda_reg=0.01,w=1,num_iter=1000,tol=1e-10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "compute_square_loss(vld_X, vld_y, wa)\n",
    "# vec form is much faster than loop form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5 [Optional] Projected SGD via Variable Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def projected_stochastic_gradient(X, y, theta_p, theta_m, lambda_reg):\n",
    "    \n",
    "    N,d = int(X.shape[0]),int(X.shape[1])\n",
    "    \n",
    "    E = np.dot(X,theta_m) + y\n",
    "    grad_p = 2 * np.dot(np.transpose(theta_p),np.transpose(X))\n",
    "    grad_p = np.dot(grad_p,X)\n",
    "    grad_p = grad_p - 2 * np.dot(np.transpose(E),X)\n",
    "    grad_p = grad_p/(int(y.shape[0]))\n",
    "    grad_p = grad_p + lambda_reg \n",
    "    grad_p = np.transpose(grad_p)\n",
    "\n",
    "    F = np.dot(X,theta_p) - y\n",
    "    grad_m = 2 * np.dot(np.transpose(theta_m),np.transpose(X))\n",
    "    grad_m = np.dot(grad_m,X)\n",
    "    grad_m = grad_m - 2 * np.dot(np.transpose(F),X)\n",
    "    grad_m = grad_m/(int(y.shape[0]))\n",
    "    grad_m = grad_m + lambda_reg \n",
    "    grad_m = np.transpose(grad_m)\n",
    "    \n",
    "    grad_p= grad_p/math.sqrt(sum(np.multiply(grad_p,grad_p)))\n",
    "    grad_m= grad_m/math.sqrt(sum(np.multiply(grad_m,grad_m)))\n",
    "            \n",
    "    return (grad_p,grad_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def projected_stochastic_gradient_descent(X, y, alpha=0.1, lambda_reg=0.1, num_iter=1000,alpha_sqrt=False,tol=1e-5):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with a regularization term\n",
    "    \n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - string or float. step size in gradient descent\n",
    "                NOTE: In SGD, it's not always a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every iteration is alpha.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t)\n",
    "                if alpha == \"1/t\", alpha = 1/t\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_iter - number of epochs (i.e number of times) to go through the whole training set\n",
    "    \n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features) \n",
    "        loss hist - the history of regularized loss function vector, 2D numpy array of size(num_iter, num_instances)\n",
    "    \"\"\"\n",
    "        \n",
    "    N, d = int(X.shape[0]), int(X.shape[1])\n",
    "    \n",
    "    theta_p = np.expand_dims(np.ones(d), axis=1) #initialize theta\n",
    "    theta_m = np.expand_dims(np.zeros(d), axis=1) #initialize theta\n",
    "    \n",
    "    g_p_lag = 0\n",
    "    g_m_lag = 0\n",
    "    \n",
    "    theta_p_hist = np.zeros((num_iter, N, d))  #Initialize theta_hist\n",
    "    theta_m_hist = np.zeros((num_iter, N, d))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros((num_iter, N)) #Initialize loss_hist\n",
    "    g_p_hist = np.zeros((num_iter, N, d))\n",
    "    \n",
    "    alpha =  alpha / (1+ alpha *lambda_reg)\n",
    "    \n",
    "    for i in range(0,num_iter):\n",
    "        \n",
    "        if alpha_sqrt==False: alpha=1.0/(i+1)\n",
    "        else: alpha=1.0/math.sqrt(i+1)\n",
    "        \n",
    "        for j in range(0,N):\n",
    "            theta = theta_p - theta_m\n",
    "            loss_hist[i][j] =  loss_hist[i][j]+ compute_lasso_square_loss(X[j:j+1], y[j:j+1], theta, lambda_reg)[0]\n",
    "            theta_p_hist[i][j] = theta_p_hist[i][j]+ np.transpose(theta_p)[0]\n",
    "            theta_m_hist[i][j] = theta_m_hist[i][j]+ np.transpose(theta_m)[0]\n",
    "            g_p,g_m = projected_stochastic_gradient(X[j:j+1], y[j:j+1], theta_p, theta_m, lambda_reg)\n",
    "            \n",
    "            g_p_hist[i][j] = g_p_hist[i][j]+ np.transpose(g_p)[0]\n",
    "            \n",
    "\n",
    "            if j > 3:\n",
    "\n",
    "                if sum(np.transpose(sum(abs(g_p+g_m+g_p_lag+g_m_lag)))*alpha)/(2*d) < tol:\n",
    "                    \n",
    "                    print (\"iteration: %s times; now on %sth instance\" % (i,j))\n",
    "                    return (theta_p_hist,theta_m_hist,loss_hist)\n",
    "                                                 \n",
    "            theta_p = theta_p-g_p*alpha\n",
    "            theta_m = theta_m-g_m*alpha\n",
    "            \n",
    "            for k in range(0,int(theta_p.shape[0])):\n",
    "                if theta_p[k] < 0:\n",
    "                    theta_p[k] = 0\n",
    "                if theta_m[k] < 0:\n",
    "                    theta_m[k] = 0\n",
    "                    \n",
    "            g_p_lag = g_p\n",
    "            g_m_lag = g_m \n",
    "            \n",
    "\n",
    "    return (theta_p_hist,theta_m_hist,loss_hist)\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 39.6700000763 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "a,b,c= projected_stochastic_gradient_descent(train_X, train_y, alpha=0.1, lambda_reg=0.01, num_iter=1000,alpha_sqrt=False,tol=1e-10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "theta = a[-1][-2:-1]-b[-1][-2:-1]\n",
    "theta = np.transpose(theta)\n",
    "compute_square_loss(vld_X, vld_y, theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "projected SGD is slower than shooting algorithm but gives better results under the same number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_f1 = find_l1_minimizer_lambda(train_X,train_y,vld_X,vld_y, theta=1, lambda_start_log = -7.0,lambda_end_log = 3.0, num_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 39.1119999886 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.14996966]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "a,b,c= projected_stochastic_gradient_descent(train_X, train_y, alpha=0.1, lambda_reg=lambda_f1, num_iter=1000,alpha_sqrt=True,tol=1e-10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "theta = a[-1][-2:-1]-b[-1][-2:-1]\n",
    "theta = np.transpose(theta)\n",
    "compute_square_loss(vld_X, vld_y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **` alpha_sqrt=True decreases alpha too fast that loss is not minimized, alpha_sqrt=False is better, but notice that fixed alpha also achieves a level as low as alpha_sqrt=True` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0469999313354 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.24152318]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "wa,wi = coordinate_descent_lasso_vec(train_X,train_y,lambda_reg=lambda_f1,w=1,num_iter=1000,tol=1e-10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "compute_square_loss(vld_X, vld_y, wa)\n",
    "# vec form is much faster than loop form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "2.0 supports against 75.0 elements in total, 0.0266666666667\n"
     ]
    }
   ],
   "source": [
    "analyze_the_sparsity(theta,tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 supports against 75.0 elements in total, 0.0\n"
     ]
    }
   ],
   "source": [
    "analyze_the_sparsity(wa,tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.0 supports against 75.0 elements in total, 0.0133333333333\n"
     ]
    }
   ],
   "source": [
    "analyze_the_sparsity(wa,tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "19.0 supports against 75.0 elements in total, 0.253333333333\n"
     ]
    }
   ],
   "source": [
    "analyze_the_sparsity(theta,tol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **` pSGD-produced sparcity is more preciser that coordinate` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
